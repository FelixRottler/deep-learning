{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "from nltk.corpus import twitter_samples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "nltk.download(\"twitter_samples\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/i540927/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "nltk.download(\"stopwords\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/i540927/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import re\n",
    "import string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#Import the english stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuation\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def process_tweet(tweet:str):\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)    \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "\n",
    "    # tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    tweets_clean = []\n",
    "\n",
    "    for word in tweet_tokens: # Go through every word in your tokens list\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove punctuation\n",
    "            tweets_clean.append(word)\n",
    "    stemmer = PorterStemmer() \n",
    "\n",
    "    # Create an empty list to store the stems\n",
    "    tweets_stem = [] \n",
    "\n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)  # stemming word\n",
    "        tweets_stem.append(stem_word)  # append to the list\n",
    "\n",
    "    return tweets_stem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, positive_tweets, negative_tweets,vectors=None,tokenize=lambda x:x.split(),pad_token=\"<pad>\",unk_token=\"<unk>\"):\n",
    "        self.tweets = []\n",
    "      \n",
    "        specials=[\"<pad>\",\"<unk>\"]\n",
    "\n",
    "        def yield_tokens(data):\n",
    "            for tweet in data:\n",
    "                tokens = tokenize(tweet)\n",
    "                yield tokens\n",
    "            \n",
    "        self.vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "            yield_tokens(negative_tweets+positive_tweets),\n",
    "            special_first=True,\n",
    "            specials=specials)\n",
    "        self.pad_idx = self.vocab[\"<pad>\"]\n",
    "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "        \n",
    "\n",
    "        for p_tweet in positive_tweets:\n",
    "            tokens = tokenize(p_tweet)\n",
    "            self.tweets.append((1, self.vocab.forward(tokens)))\n",
    "        for n_tweet in negative_tweets:\n",
    "            tokens = tokenize(n_tweet)\n",
    "            self.tweets.append((0, self.vocab.forward(tokens)))\n",
    "       \n",
    "       \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tweets[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "            # batch in that case is List of batches that contain the elements of the iterator\n",
    "        text_list = []\n",
    "        target_list=[]\n",
    "        len_list=[]\n",
    "        \n",
    "        for (label,x) in batch:\n",
    "            x = torch.tensor(x, dtype=torch.int64)\n",
    "            text_list.append(x)  \n",
    "            target_list.append(label)\n",
    "            len_list.append(len(x))\n",
    "\n",
    "        len_list = torch.tensor(len_list,dtype=torch.int64)\n",
    "        sorted_lens, sorted_idx = torch.sort(len_list,descending=True)\n",
    "        target_list = torch.tensor(target_list, dtype=torch.int64)[sorted_idx]\n",
    "        text_list = pad_sequence(text_list, batch_first=True)[sorted_idx]\n",
    "        \n",
    "        return text_list,target_list, sorted_lens\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "positive_tweets = twitter_samples.strings(\"positive_tweets.json\")\n",
    "negative_tweets = twitter_samples.strings(\"negative_tweets.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "tweet_dataset = TweetDataset(positive_tweets,negative_tweets,tokenize=process_tweet)\n",
    "\n",
    "\n",
    "n_train = int(0.8*len(tweet_dataset))\n",
    "n_test = len(tweet_dataset)- n_train\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(tweet_dataset,[n_train,n_test])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=TweetDataset.collate_fn,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset ,batch_size=64,collate_fn=TweetDataset.collate_fn)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# TODO add comments to explain what is happening\n",
    "class TweetClassification(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, \n",
    "                        input_size, \n",
    "                        hidden_size,\n",
    "                        output_size,\n",
    "                        bidirectional=True,\n",
    "                        padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pad_idx = padding_idx\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size,input_size,padding_idx=padding_idx)\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size,\n",
    "                                    hidden_size=hidden_size, \n",
    "                                    bidirectional=bidirectional,\n",
    "                                    batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = torch.nn.Linear(in_features = 2*hidden_size,out_features=hidden_size)\n",
    "        else:\n",
    "            self.fc = torch.nn.Linear(in_features = hidden_size,out_features=hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(in_features=hidden_size,out_features=output_size)\n",
    "\n",
    "    def forward(self, seq, lens):\n",
    "        # batch first sequence seq (batch_size, seq_length)\n",
    "        x = self.dropout(self.embeddings(seq))\n",
    "        # After embedding layer the dim is bach_size, seq_length, embedding_dim\n",
    "        # enforce_sorted is a problem because it changes the ordering, then the logits and the target ordering is not the same anymore?\n",
    "        x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, lens, batch_first=True)\n",
    "        x,(h_n,c_n) = self.lstm(x_packed)\n",
    "        x,x_lens = torch.nn.utils.rnn.pad_packed_sequence(x,batch_first=True)\n",
    "     \n",
    "        #x = torch.cat([h_n[0,:,:],h_n[1,:,:]],dim=1)\n",
    "        \n",
    "        x = x[range(x.shape[0]),x_lens-1,:]\n",
    "        x = self.dropout(self.fc(x))\n",
    "     \n",
    "        return torch.sigmoid(self.fc2(x))\n",
    "\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, device, num_epochs=25):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(\"-\"*10)\n",
    "\n",
    "        for phase in [\"train\",\"test\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            running_loss = 0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for texts,targets,lens in dataloaders[phase]:\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase==\"train\"):\n",
    "                    x = texts.to(device)\n",
    "                    y = targets.to(device)\n",
    "\n",
    "                    outputs = model(x,lens).squeeze()\n",
    "                    loss = criterion(outputs,y.float())\n",
    "                \n",
    "                    if phase== \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * x.shape[0]\n",
    "                running_corrects+= torch.sum((torch.round(torch.sigmoid(outputs)) ==  y)).item()\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_accuracy = running_corrects / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_accuracy))\n",
    "    print()\n",
    "        \n",
    "input_size= 200\n",
    "hidden_size = 256\n",
    "vocab_size = len(tweet_dataset.vocab)\n",
    "output_size = 1\n",
    "bidirectional = True\n",
    "\n",
    "crit = torch.nn.BCELoss()\n",
    "net = TweetClassification(vocab_size,\n",
    "                            input_size,\n",
    "                            hidden_size,\n",
    "                            output_size,\n",
    "                            padding_idx=tweet_dataset.pad_idx,\n",
    "                            bidirectional=bidirectional)\n",
    "optim = torch.optim.Adam(net.parameters())\n",
    "\n",
    "EPOCHS=30\n",
    "dataloaders ={\"train\":train_loader,\"test\":test_loader}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "train_model(net,dataloaders,crit,optim,device,num_epochs=EPOCHS)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0/30\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 0.5675\n",
      "test Loss: 0.0225 Acc: 0.5065\n",
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 0.0143 Acc: 0.5867\n",
      "test Loss: 0.0196 Acc: 0.8285\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 0.0074 Acc: 0.7274\n",
      "test Loss: 0.0185 Acc: 0.8555\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.0056 Acc: 0.7462\n",
      "test Loss: 0.0168 Acc: 0.8500\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 0.0061 Acc: 0.6707\n",
      "test Loss: 0.0232 Acc: 0.8120\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.7147\n",
      "test Loss: 0.0241 Acc: 0.8435\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.7561\n",
      "test Loss: 0.0217 Acc: 0.9400\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.8317\n",
      "test Loss: 0.0255 Acc: 0.8940\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.8461\n",
      "test Loss: 0.0208 Acc: 0.9725\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 0.0021 Acc: 0.8950\n",
      "test Loss: 0.0646 Acc: 0.9755\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.8191\n",
      "test Loss: 0.0658 Acc: 0.9535\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 0.0255 Acc: 0.9049\n",
      "test Loss: 0.0618 Acc: 0.9905\n",
      "Epoch 12/30\n",
      "----------\n",
      "train Loss: 0.0020 Acc: 0.9631\n",
      "test Loss: 0.0606 Acc: 0.9300\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 0.0052 Acc: 0.8444\n",
      "test Loss: 0.0634 Acc: 0.8595\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.7454\n",
      "test Loss: 0.0150 Acc: 0.8845\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.7525\n",
      "test Loss: 0.0141 Acc: 0.9095\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.8350\n",
      "test Loss: 0.0173 Acc: 0.9155\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.8796\n",
      "test Loss: 0.0627 Acc: 0.9635\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9263\n",
      "test Loss: 0.0636 Acc: 0.9860\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9661\n",
      "test Loss: 0.0649 Acc: 0.9895\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9489\n",
      "test Loss: 0.0627 Acc: 0.9685\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9460\n",
      "test Loss: 0.0650 Acc: 0.9785\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9533\n",
      "test Loss: 0.0668 Acc: 0.9920\n",
      "Epoch 23/30\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.9549\n",
      "test Loss: 0.0613 Acc: 0.9895\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 0.0013 Acc: 0.9731\n",
      "test Loss: 0.0661 Acc: 0.9815\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9185\n",
      "test Loss: 0.0621 Acc: 0.9755\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.9631\n",
      "test Loss: 0.0656 Acc: 0.9830\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9210\n",
      "test Loss: 0.0622 Acc: 0.9770\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 0.0001 Acc: 0.9513\n",
      "test Loss: 0.0651 Acc: 0.9885\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9709\n",
      "test Loss: 0.0653 Acc: 0.9880\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "i = 1010\n",
    "print(negative_tweets[i])\n",
    "x=tweet_dataset.vocab.forward(process_tweet(negative_tweets[i]))\n",
    "x = torch.tensor(x).unsqueeze(0)\n",
    "lens = torch.tensor([len(x)],dtype=torch.int64)\n",
    "net.eval()\n",
    "net(x,lens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Oh my god :(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.0604]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('dl': conda)"
  },
  "interpreter": {
   "hash": "56f069e7f80ff7150fb1516d40bbe80842093e730232ba1b152d381f375fd298"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}